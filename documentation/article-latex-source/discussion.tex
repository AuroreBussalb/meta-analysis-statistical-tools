% adding the line below for Multifile document support with LatexTools Sublime package 
%!TEX root = manuscript.tex


% Discussion

\section{Discussion}

\subsection{Meta-analysis} 

This replication and update of a meta-analysis did not meet all PRISMA recommendations \citep{Moher2009}. In particular, the risk of bias
in individual and across studies was not assessed. 

In the meta-analysis performed here, we challenged some choices made by \citeauthor{Cortese2016}, which proved controversial: 
the computation of between-\gls{es} based on an unusual scale \citep{Steiner2014} and the inclusion of a pilot study \citep{Arnold2014} 
whose endpoint values were not available at the time \citeauthor{Cortese2016} conducted their meta-analysis. We here review the 
list of changes, their justification, and their impact on the analysis.
 
First, relying on the Conners-3 \citep{Conners2011} instead of the BOSS Classroom Observation \citep{Shapiro2010} for
teachers' ratings seems preferable because this scale is more commonly used \citep{Christiansen2014, Bluschke2016} and is
a revision of the Conners Rating Scale Revised \citep{Conners1998} whose reliability has been studied \citep{Collett2003}. 
However, relying on one or the other scale did not change the significance of the between-\gls{es}, regardless of outcome.

Second, to compute the between-\gls{es} of \citet{Arnold2014} the clinical scores taken when all sessions were completed were 
used instead of looking at interim results as with \citeauthor{Cortese2016}. Some studies suggested that the number of sessions 
correlates positively with the changes observed in the \gls{eeg} \citep{Vernon2004} so that a lower number of sessions would 
lead to artificially smaller between-\gls{es}. Here, the between-\gls{es} computed with the values at post test of \citet{Arnold2014} were smaller 
than those obtained after 12 sessions; however, these differences did not lead to a change of significance of the \gls{se}. 

To conclude on this meta-analysis, although some points were controversial, the impact on the
meta-analysis was minimal and did not change the statistical significance of any outcome. 
The addition of the \textcolor{red}{three} new studies \citep{Strehl2017, Baumeister2016, Bazanova2018} further confirmed the original results. Indeed, the
significance did not change for any outcome: the \gls{se} remained significant for \gls{mprox} raters and
non-significant for \gls{pblind}. Adding \textcolor{red}{three} more studies increased the significance of the sensitivity analysis run by
\citeauthor{Cortese2016}, most notably the \gls{se} of studies corresponding to \gls{nfb} "standard protocols" \citep{Arns2014}. 
While \citeauthor{Cortese2016} found that this subset tends to perform better, particularly on the \gls{pblind} outcome, 
\textcolor{red}{adding two studies confirmed this result on the total clinical score} (p-value < 0.05). Despite the obvious heterogeneity 
of the studies included in this subset (particularly in terms of protocol used), these results suggest a positive relation 
between the features of this \emph{standard} design and \gls{nfb} performance. This result is a breakthrough in the demonstration 
of standard \gls{nfb} protocol efficacy for the treatment of \gls{adhd}. Nonetheless, the  studies 
included in this subset are still highly heterogeneous (particularly in terms of protocol used), a factor which should be accounted for.


\subsection{Factors influencing neurofeedback}

Description and analysis of different types of \gls{nfb} implementation were subject to several studies \citep{Arns2014, 
Enriquez2017, Vernon2004, Jeunet2018, Arns2009, Cortese2016}. \textcolor{red}{However, to the best of our knowledge, none of these studies has 
implemented a systematic multivariate approach to associate factors to clinical endpoints therefore exposing their univariate analysis to a 
greater extend to the presence of a confounding factor.}

\textcolor{red}{Two observations were detected as outliers and so removed from the dataset before performing the \gls{saob}: }\citeauthor{Bazanova2018}'s \textcolor{red}{individualized \gls{nfb} and 
individualized \gls{nfb} coupled with \gls{emg}-Biofeedback groups. Indeed, these two groups presented very large within-\gls{es} (-3.41 and -3.95), even bigger than those reported in the literature on medication treating 
\gls{adhd}} \citep{Luan2017}\textcolor{red}{. These large values broke our working hypothesis, so in order to be able to conclude on the results obtained by the \gls{saob}, an outlier 
rejection was implemented.}

\textcolor{red}{As expected, the number of sessions was found to be significant, even if it was by only two methods, which was in compliance with existing literature}.
\textcolor{red}{For instance, using several univariate regressions without correction for multiple testing} \citet{Arns2009}, 
\citet{Arns2014} stated that performing less than 20 \gls{nfb} sessions leads to smaller effects. Similarly, \citet{Vernon2004} 
observed that positive changes in the \gls{eeg} and behavioral performance occurred after a minimum of 20 sessions. However, \citet{Enriquez2017} 
insisted that the number of sessions should be carefully chosen in order to avoid "overtraining". \textcolor{red}{The fact that the number of sessions was not 
identified by the \gls{lasso}} as a positively contributing factor might be explained by the presence of only two data points with 20 sessions or less. 
Conceivably, the temporal threshold of efficacy was passed for all included studies, making the identification of this factor by the three methods unlikely on 
this dataset. \textcolor{red}{However, the two methods that identified this factor both agreed that as expected, the more sessions performed, 
the more efficient the \gls{nfb} tends to be.}

Interestingly, \citep{Minder2018} suggests that the subject location of the \gls{nfb} training may also be an important contributory 
factor to clinical effectiveness. However, this has been challenged by a recent study \citep{Minder2018} showing that 
performing \gls{nfb} at school or at the clinic has no significant impact on treatment response. 

The type of \gls{nfb} protocol was not identified by \textcolor{red}{any} method, and did not appear to influence the \gls{nfb} results. 
This minimal importance granted by the methods to the \gls{nfb} protocols is counter-intuitive given the centrality of the 
protocols in the neurophysiological mode of action and subsequent expected impact on
therapeutic effectiveness \citep{Vernon2004}. A possible explanation for this result is that these protocols were equally 
efficacious for the populations to whom they were offered and thereby did not constitute a significant explanatory factor. 
This result, however, does not preclude a combined and personalized strategy (offering personalized protocols based on phenotypes)
to further improve performance, as previously suggested by \citet{Alkoby2017}.

Several factors were selected by all three methods with the same direction of influence: the EEG quality, the treatment 
length, and the rater's probably blindness to the treatment. First, our analysis highlighted that recording \gls{eeg} 
in good conditions leads to better results.
This can be explained by the fact that better signal quality enables more accurate extraction of \gls{eeg} patterns
linked to \gls{adhd} and hence leads to better learning and therapeutic efficacy \citep{Congedo2004}. 
However, it remains difficult to assess the quality of \gls{eeg} hardware (such as the amplifier used) 
because little information is provided in these studies.  
This calls for greater care in future studies, which should strive to assess and report the quality of the data.

Next, it appears that the longer the course of treatment, the less efficient it becomes. \textcolor{red}{This may be explained by the degree of 
engagement with \gls{nfb} intervention: it may be harder to be engaged with a long course of treatment. However, it is difficult to quantify because 
either no questionnaires assessing engagement  were submitted to children or this information was not provided. It is an interesting point to investigate,
so we invite future studies to share it if possible.}

Arguably, the treatment length is a proxy for treatment intensity, suggesting that a shorter period of treatment is more likely to succeed because the frequency of the sessions
is higher. This hypothesis is supported by the fact that the variable \emph{session pace} (number of
sessions per week) is also associated with larger within-\gls{es} according to the \gls{lasso} and the decision tree. The impact of the
intensity of treatment has been investigated by \citep{Rogala2016} on healthy adults: it was observed that studies with
at least four training sessions completed on consecutive days were all beneficial. Overall, these results suggest adopting a high session pace, 
which is not common knowledge in the field.

\textcolor{red}{Some other factors' influence would have been interesting to investigate, such as using personalized \gls{nfb} protocols based on the 
\gls{iapf}} \citep{Liu2016}, \textcolor{red}{which seems promising according to} \citet{Bazanova2018, Escolano2014}. \textcolor{red}{However, it could not be included in the 
\gls{saob} because only two studies used personalized \gls{nfb} protocols. This lack of studies is also the reason why the impact of coupling \gls{emg}-Biofeedback with \gls{nfb} could not be 
included in the \gls{saob}. Another interesting factor, which could have helped explain the result on the treatment duration, was excluded from the analysis: 
the severity at baseline. Although pre-test scores were available for each study, they could not be compared because they were measured on different scales. 
To address this problem, the scores were normalized using the maximum score to be obtained on each scale. However, this value was not found for several clinical scales, 
which led to missing observations. When more studies including these features will be available, it would be interesting to run the \gls{saob} to determine their influence.} 

In general our results strongly support the effectiveness of \gls{nfb} for the treatment of \gls{adhd}. However, as expected, 
the assessment of symptoms by non-blind raters leads to far more favorable results than by \gls{pblind} raters, a result 
widely expected and in close compliance with the existing meta-analysis \citep{Cortese2016, Micoulaud2014}. 
This observation would certainly be contradictory should teachers’ 
assessments reflect a placebo effect, which has long been documented in the literature \citep{Sollie2013, Narad2015, Minder2018}. 
This point is investigated in greater detail in the following section.

\subsection{Analysis on the probably blind raters}

Teachers were considered as \gls{pblind} raters by \citeauthor{Cortese2016} and \citeauthor{Micoulaud2014}.
Unexpectedly, the data provided did not exactly match the widely accepted hypothesis stating that the difference between
\gls{mprox} and \gls{pblind} can solely be explained by the placebo effect. 
Nonetheless, the emphasis put on 'probably' indicates that teachers may be aware of the treatment followed. 
An element that corroborates this hypothesis is the fact that, for all the studies included in this work, the amplitude 
of the clinical scale at baseline suggests that teachers did not capture the full extent of the symptoms or, put differently, 
that they were blind more to the symptoms than to the intervention, as illustrated 
in Figure~\ref{Figure:discussion_on_placebo_effect_colors_2-columns_fitting_image}. 
Indeed, before the intervention, teachers rated the symptoms less severely compared to parents and observed less improvement at post-test: 
this tends to correspond more to case A with no placebo effect than case B. The expected differences of ratings between 
teachers and parents have been extensively studied \citep{Sollie2013, Narad2015, Minder2018}, observing that teachers are more 
likely to underrate a child's symptom severity, especially for younger children. As a consequence, teachers might simply be less likely 
to observe a clinical change over the course of the treatment \citep{Sollie2013, Narad2015, Minder2018}. Moreover, it is also clear 
that there is more variability in teachers' scores compared to parents', which could partly explain the lower \gls{es} obtained for 
\gls{pblind} raters, since the variability deflates the \gls{es}. In conclusion, using \gls{pblind} as an estimate for correcting the 
placebo effect does not appear an appropriate choice. 

Another way to highlight a possible placebo effect is to focus on the decision tree illustrated in Figure
~\ref{Figure:factors_analysis_decision_tree_results}.
The top node splits: on the one hand \textcolor{red}{46} observations corresponding to \gls{mprox} raters and, on the other, 
21 observations corresponding to \gls{pblind}. If the differences observed between \gls{pblind} and \gls{mprox} raters were 
due to the placebo effect, one would expect to find in the \gls{mprox} sub-tree some factors linked to the perception
of the implication in the treatment. This was indeed the case: \textcolor{red}{treatment length was} found to be significant but not in the
direction corroborating the hypothesis that they are a part of a placebo effect. Indeed, one would expect that the
longer the treatment, the higher the placebo effect and the greater the within-\gls{es}. Instead, the opposite was found, 
somewhat invalidating the hypothesis. 

Overall, these results suggest that \gls{pblind} assessments could hardly be used to assess placebo effect as they seem to be blinder 
to symptoms than to intervention. In the absence of an ethically \citep{Holtmann2014} and technically \citep{Birbaumer1991} feasible sham 
for \gls{nfb} protocols \citep{World-Medical-Association2000}, it is necessary to fall back on an acceptable methodological alternative for 
the demonstration of clinical effectiveness. Among those are the analysis of neuromarkers collected during \gls{nfb} treatment demonstrating 
that patients do \emph{control} the trained neuromarkers; that they \emph{learn} (reinforce control over time), and that these possibly 
lead to lasting brain reorganization (e.g., changes in their baseline resting state activity). The specificity of these changes, in relation 
to which neuromarkers were trained and to the clinical improvement, will be an essential component of this demonstration.  

% words number = 1592
