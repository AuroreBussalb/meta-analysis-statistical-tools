% adding the line below for Multifile document support with LatexTools Sublime package 
%!TEX root = manuscript.tex


% Discussion

\section{Discussion}

\subsection{Replicate and update a meta-analysis}

In this replication and extension of \citet{Cortese2016}, we investigated choices made by \citeauthor{Cortese2016}, 
which proved controversial: the computation of \gls{es} based on an unusual scale \citep{Steiner2014} and the inclusion 
of a pilot study \citep{Arnold2014} whose final results weren't available at the time \citeauthor{Cortese2016} 
conducted his meta-analysis. We review here the list of changes, their justification, and their impact on the analysis.
 
First, relying on the Conners-3 \citep{Conners2008} instead of the BOSS Classroom Observation \citep{Shapiro2010} 
for teachers ratings seems preferable because this scale is more often used \citep{Christiansen2014, 
Bluschke2016} and is the revision of the Conners Rating Scale Revised \citep{Conners1998} whose reliability has been studied 
\citep{Collett2003}. However, relying on one or the other scale did not 
change the significance of the s\gls{es} whatever the outcome studied.

Second, to compute the \gls{es} of \citet{Arnold2014} we used the values at post-test
whereas \citeauthor{Cortese2016} calculated it with ratings 
after only 12 sessions of \gls{nfb}. Some studies suggest that the number of sessions positively 
correlates with the changes in the \gls{eeg} \citep{Vernon2004} so a lower number of sessions would lead to 
artificially smaller \gls{es}. Here, the \gls{es} computed with the values at post test of \citet{Arnold2014} 
are smaller than those obtained after 12 sessions but these differences do not lead to a change of significance of the \gls{se}. 

Finally, we found that some of the studies included in \citeauthor{Cortese2016}'s work 
\citep{Arnold2014} and \citep{Steiner2011} were described by their respective authors as \emph{pilot} 
studies and it was disclosed in \citet{VanDongen2013, vanDongenBoomsma2015} \begin{quote} [they were] unable to recruit a sufficient 
number of participants to meet [their] planned sample size .\end{quote} Nevertheless not including them would introduce bias.

To conclude on the replication of \citet{Cortese2016}, though some of the choices made by authors
were controversial and the fact that - for the reasons mentioned earlier - different choices could reasonably be made, 
it turns out that the impact on the meta-analysis results are minimal and do not change the statistical significance of any outcome. 
Consequently, the completion of the meta-analysis with studies published since the publication of his work were done with the choices: 
\begin{description} 
	\item To compute the \gls{es} of \citet{Arnold2014} the values at post-test were used.
	\item The scores reported by teachers on the Conners-3 in \citeauthor{Steiner2014}'s study were taken into account instead of these of 
	the BOSS Classroom Observation.
	\item Results were obtained with the Python code.
\end{description} 

The addition of the two new studies \citep{Strehl2017, Baumeister2016} further confirms those results. Indeed, 
the significance does not change for any outcome: \gls{se} found remains significant for parents' scores and 
non-significant for teachers. 

Adding two more studies increases the significance of the sensitivity analysis ran by \citeauthor{Cortese2016}. 
Most interestingly, the \gls{es} from the subset of studies corresponding to standard protocols of \gls{nfb} as 
defined by \citet{Arns2014}. While \citeauthor{Cortese2016} found that this subset tend to perform better, particularly
 on the \gls{pblind} outcome, adding two studies confirms this result on the total score with a p-value of 0.043. 
Despite the obvious heterogeneity of the studies included in this subset (particularly in terms of protocol used), 
this result suggests a positive relation between the features of this \emph{standard} design and \gls{nfb} performance.

Eventually, concerning the raters, we considered teachers as \gls{pblind} raters as \citeauthor{Cortese2016} and 
\citeauthor{Micoulaud2014} did although they may be aware of the treatment followed thanks to the parents. 
Besides, the amplitude of the clinical scale at baseline suggests that teachers do not capture the full picture of 
the condition and are therefore less likely to see a change (prone to type II error).

Along with this article, the Python code and raw data are provided in order to facilitate a potential replication of this work
(available on the Github repository).  

% 573 words

\subsection{Identify factors influencing the Neurofeedback}

Description and analysis of \gls{nfb} implementation was subject to several studies \citep{Arns2014, Enriquez2017, Vernon2004} 
but to our knowledge none used statistical tools to detect the influence of methodological, clinical and technical factors 
on such a wide range of studies. 

A somewhat puzzling result is the fact that the three methods which offer to identify factors contributing to the \gls{nfb} 
performance do not lead to the exact same results. These discrepancies are clearly explained by the varying hypothesis 
of these models and actually offer interesting insight into the results and their significance. For instance, the decision tree method is non 
linear and accounts for variables interaction which is not the case for the two others methods. Moreover, the decision tree is unstable 
\citep{dwyer2007}, meaning that a small change in the data can cause an important change in the structure of the optimal decision tree.

Nevertheless, despite these differences between the methods, 3 factors are consistently identified by all the methods with 
the same influence direction: if the rater is probably blind to the treatment, the treatment length, and the EEG quality. 

As expected, the assessment of symptoms by non-blind raters leads to more favorable results than by blind raters, 
result observed in several meta-analysis  \citep{Cortese2016, Micoulaud2014}. If the differences observed between blind 
and non-blind raters are due to the placebo effect, the part of the decision tree where there is only observations of 
non-blind raters may enable us to detect the factors affected by the placebo effect. 
Thus, two factors linked to the perception of the treatment (treatment length and session length)
are present in this part of the tree whereas no such factors are returned for 
the \gls{pblind} part of the tree.

The treatment length varied more than the session pace and the age bounds of included children between the included studies as shown 
by the boxplot of standardized values \cref{Figure:factors_analysis_boxplots}, so detecting it as an influencing factor may have 
been easier. It appears here that the longer the treatment the less efficient it becomes. Arguably, the treatment length is a proxy 
for treatment intensity, which means that a treatment that is short in length (and consequently intense in pace) 
is more likely to succeed. This hypothesis is back-up by the fact that the variable \emph{session pace} (number of sessions per week) 
is also associated with larger \gls{es} according to the \gls{wls} and \gls{lasso}. 

Eventually, this analysis points out the fact that recording \gls{eeg} in good conditions seems to lead to better results, 
which can be explained by the fact that better signal quality enables to extract more correctly the \gls{eeg} 
patterns linked to \gls{adhd} and henceforth leads to better learning and therapeutic efficacy. However, it remains difficult to 
really assess the quality of the hardware because little information is provided in the studies.  

The interpretation of factors returned only by two methods was less reliable. Keeping that in mind, it was interesting 
to study the direction of their effect though: an \gls{irb} approval seems preferable implying that a well-conducted study 
seems lead to favorable results; a high session pace, so a more intensive training, appears to lead to better results,
and the artifact correction based on amplitude do not seem to be an appropriate method to remove the artifacts and the \gls{smr} protocol
appears not to be the best approach. 

Surprisingly, the number of sessions is not found as an significant influencing factor by any method, 
which is somewhat in contradiction with existing literature.
For instance, \citet{Enriquez2017} insisted on the fact that the number of sessions should be 
chosen carefully to avoid "overtraining". Moreover, \citet{Arns2014} stated that performing less than 20 \gls{nfb} sessions 
lead to smaller effects. Indeed {Vernon2004} observed that positive changes in the \gls{eeg} and behavioral 
performance occurred after a minimum of 20 sessions, but he also points out the fact that the
location of the \gls{nfb} training may had an important influence. 
Nevertheless, in our study, regardless of the significance of the number of sessions, the coefficient 
found by the \gls{wls} is negative, meaning that as expected,
the more sessions performed the more efficient the \gls{nfb} seem to be. 

The type of protocol is not identified by all the three methods but it seems to influence the \gls{nfb} results in particular the the theta down 
which appeared more efficient and \gls{smr} which conversely seems associated with lower \gls{es}. We expected 
more precised results on the protocols criteria because this point is central in \gls{nfb} as pointed out by \citet{Vernon2004}.
A possible explanation is that all these protocols are equally efficacious to the populations they were offered to and 
thereby do not constitute a significant explanatory factor. This result, however,
does not preclude a combined and personalized strategy (offer the right protocol to the right kid) to 
further improve performance. 

It would have been interesting to study the influence of some other factors such as the delay between brain state and feedback signal as well as the type of \gls{nfb} game used 
but these information are rarely available in studies and also . Besides, to add more reliability to these results it should be preferable to add more studies, 
particularly studies with teachers assessments (considered as \gls{pblind}). 

% words number = 849