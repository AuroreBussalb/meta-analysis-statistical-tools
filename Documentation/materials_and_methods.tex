% adding the line below for Multifile document support with LatexTools Sublime package 
%!TEX root = manuscript.tex

% Materials and Methods

\section{Materials and Methods}

\subsection{Studies selection}

Search terms were entered in Pubmed and studies included in previous meta-analysis were identified. Among these studies, those which
satisfied each of these points were selected:
\begin{itemize}
	\item assess \gls{nfb} efficacy; 
	\item subjects must be diagnosed \gls{adhd} based on DSM-IV \citep{DSM-4}, DSM-V \citep{DSM-5}, ICD-10 \citep{ICD101993} 
	criteria or according to an expert psychiatrist; 
	\item language is English, German or French;
	\item include at least 8 subjects in each group;
	\item subjects must be younger than 25 years old;
	\item provide enough data to compute required metrics for the following analysis.
\end{itemize} 
The studies satisfying all these points were included in the \gls{saob}, then in order to replicate and 
update \citeauthor{Cortese2016} meta-analysis, we apply the inclusion criteria of this meta-analysis to the found studies. 

\subsection{Outcome definition} 

In included studies, the severity of \gls{adhd} symptoms have been assessed by parents and, when available, by teachers. \citet{Cortese2016} 
and \citet{Micoulaud2014} defined parents as \gls{mprox} raters who are not blind to the treatment of their child, as opposed to 
teachers who are considered as \gls{pblind} raters. This distinction is meant to assess the amplitude of the placebo effect where 
it is hypothesized that teachers who are presumed more blind to the intervention are less influenced in their assessment by their perception of it. 
Efficacy of \gls{nfb} was given for the following outcomes on clinical scales when available: inattention, 
hyperactivity/impulsivity and total scores. The factors analysis was performed using the total score only.

\subsection{Meta-analysis}

Meta-analysis are typically used to aggregate results from different clinical investigations and offer a consolidated 
state of the evidence. To aggregate results from different studies, it is necessary to assume some level of homogeneity 
in their design relative to: the inclusion criteria, the intervention, the presence, and type of control.
Because studies occasionally use slight variations of a clinical scales and because the populations and 
control groups might vary in their nature, the scores are typically standardized before being pooled. 
The between \gls{es} is one of such standardized metrics, which we implemented in this paper as described 
in the Supplemental Materials \citep{add exact reference here}. The work was carried with an open-source 
Python package developed for this work that offers a more transparent approach to the choice or parameters 
and increases replicability. This package was benchmark against RevMan v5.3 \citep{RevMan}
by replicating \citet{Cortese2016}'s work and obtaining the same results. The code is made fully available 
on a GitHub repository \cite{add exact reference here} including raw data for everyone to review its implementation, update it, and 
use it for different projects. 
 
Before updating the \citet{Cortese2016} work with recently published clinical work meeting his inclusion criteria 
\citep{Strehl2017, Baumeister2016}, we decided to run a sensitivity analysis investigating choices that later 
proved controversial \citep{Micoulaud2016}. Altogether, the changes investigated in our update included the following:
\begin{itemize}
\item the \gls{es} of \citeauthor{Arnold2014} study is computed from the post-test clinical values taken after the completion of the 40 sessions 
contrary to \citet{Cortese2016} who used the results after 12 sessions of \gls{nfb} because final values were not available;
\item the \gls{es} computed from teachers' assessments for \citet{Steiner2014} rely on the BOSS Classroom Observation \citep{Shapiro2010} whereas 
another scale more often used \citep{Christiansen2014, Bluschke2016} and which is the revision of the Conners Rating Scale Revised \citep{Conners1998} 
and whose reliability has been studied \citep{Collett2003} was provided. Thus we decided to compute the \gls{es} based on the results from the Conners.  
\end{itemize} 

Eventually, the new studies meeting the inclusion criteria defined by \citeauthor{Cortese2016} were added to the replication of the meta-analysis. 

As initially suggested we performed two subgroups analysis: first, \gls{se}, the weighted average of all the \gls{es}, was calculated with only studies following 
standard protocol as defined by \citet{Arns2014} and second with studies whose participants take low-dose or no medication during the trial. 
These analysis were performed with the choices described above. 

\subsection{Detect factors influencing the Neurofeedback}

While revisiting the work carried on meta-analysis, it became apparent that the studies pooled together where highly heterogeneous 
in terms of methodological and practical implementation. For instance, all \gls{nfb} interventions were pooled together irrelevant to the 
quality of the acquisition, the level excreted on real time data quality, and the trained neuromarker. 
Likewise, the methodological implementations varied significantly, requiring the 'subgroup' analysis (low drug, standard protocols) 
that are somewhat arbitrary. To circumvent these limitations, we implemented a novel approach taking advantage of the studies heterogeneity 
rather than suffering from it. In this setting, the within-\gls{es} of each intervention is considered as a dependent variable that
methodological and technical biases try to explain using standard statistical tools. The results of such analysis should enable us to identify 
known methodological biases (e.g.\ blind assessments negatively associates with \gls{es}) 
and possibly technical factors (e.g.\ a good control on real time data quality influences positively the treatment outcome). 

\subsubsection{Identify and pre-process factors}

We classify the factors influencing the efficacy of \gls{nfb} in 5 categories: methodological, technical, characteristics of the included
population, representative of the quality of acquisition and of the signal. 
Factors were chosen based on what was typically reported in the literature and presumed to influence \gls{es}.
%\comment{you may want to prefer `description' rather than itemize.}
\begin{itemize}
\item \emph{signal quality}: the ocular artifacts correction and the artifact correction based on amplitude; 
\item \emph{population}: the psychostimulants intake during \gls{nfb} treatment and the age bounds of children;
\item \emph{methodological biases}: the presence of a control group, the blinding of assessors, 
the randomization of subjects, and the approval by an \gls{irb};
\item \emph{\gls{nfb} implementation}: the protocol used (\gls{scp}, \gls{smr}, 
theta up, beta up in frontal areas, theta down), the presence of a transfer phase during \gls{nfb} training, the possibility to train at home 
or at school with a transfer card reminding of the \gls{nfb} session, 
the type of thresholding reward, the number of \gls{nfb} sessions, the sessions frequency during a week, the session length and the treatment length;
\item \emph{quality of acquisition}: the presence of one or more active electrode and the \gls{eeg} quality. 
This last factor was an indicator between 1 and 3: if \gls{eeg} was recorded and processed in poor conditions then the indicator would be 1. 
Besides, if the article didn't precise the recording conditions, the factor would be set to 1. To get an indicator bigger than 1, several 
points had to be satisfied:
\begin{itemize}
  \item \emph{the type of electrodes used}: \gls{agcl}/Gel and \gls{au}/Gel;
  \item \emph{check of the electrode contact quality trough the amplifier impedance acquisition mode}: inter-electrode impedance must be smaller than $40k \Omega$;  
  \item \emph{the amplifier used}: those that are conformed to European norms (such as Thera Prax \textregistered 
Neuroconn and Eemagine EE-430) are preferable or whose reliability is known.
\end{itemize}
\end{itemize}

We provide in the Github repository \ref{} the raw data extracted from the publications. To prevent any bias, the names of these factors
were hidden during the entire analysis so that the data scientist (AB) was fully blind to them. The variable names were only revealed once the data 
analysis and results were accepted as valid: this included choice of variable normalization and validation of model hypothesis as detailed below.

The pre-processing of factors for the analysis included the following steps: factors for which there were too many missing observations, 
arbitrarily set to more than 20\% of the total of observations, were removed from the analysis. Furthermore, if a factor had more than 
80\% similar observations it was removed as well. Categorical variables were coded as dummies meaning that the presence of the factor was represented by a 1 
and its absence by 0. All variables were standardized, except when the decision tree was performed. 

%\comment{be consistent with the use of time in your manuscript. I would use past tense for introduction and  materials and methods. Present for the rest.}

\subsubsection{Associate independent factors to effect sizes}

To compute this \gls{es}, means of total \gls{adhd} score given by parents and teachers were used. Besides, in case studies provided results 
for more than one behavioral scale, \gls{es} were computed for each one as 

\begin{equation}
\label{eq:factors_effect_size_within_subject}
\text{ES} = \frac{M_{\text{post},T} - M_{\text{pre},T}}{\sqrt{\frac{\sigma_{\text{pre},T}^2 + \sigma_{\text{post},T}^2}{2}}},
\end{equation} 
where $M_{t,T}$ is the mean of clinical scale taken at time $t$ (pre-test or post-test) and for treatment $T$ and $\sigma$ similarly represents its standard deviation.

The \gls{es} computed in this analysis was different from the one 
used previously for the replication and updating of \citet{Cortese2016}. Indeed, here we focused on the effect of the treatment within 
a group as defined by \citet{Cohen1988}, definition of the \gls{es} that was already used in the literature \citep{Arns2009, Maurizio2014, 
Strehl2017}. This \gls{es} enables to quantify the efficacy of \gls{nfb} inside the treatment group 

The \gls{es} was then considered as a dependent variable to be explained by the factors identified using the following three methods, which were 
implemented in the Scikit-Learn Python \citep[0.18.1]{Pedregosa2011} and the Statsmodels Python \citep[0.8.0]{Seabold2010} libraries:
\begin{itemize}
	\item weighted multiple linear regression (\gls{wls}) \citep{Montgomery2012}; 
	\item sparsity-regularized linear regression with \gls{lasso} \citep{Tibshirani1996};
	\item decision tree \citep{Quinlan1986}.
\end{itemize}

The most often used linear regression analysis is the \gls{ols} but here we applied the \gls{wls} as described in \cref{eq:factors_model_WLS}: a 
weight was assigned to each observation in order to take into account the fact that some observations came from the same study because studies 
may provide several scales. Besides, the weight was a function of the sample size as well: because of their different sample sizes,
studies were not equivalent and should be analyzed accordingly. That's why the weight corresponded to the ratio between the experiment group's sample size of the study and 
the number of behavioral scales available in the study. We also ran the analysis with \gls{ols} method to assess the impact of the weights on the results. 

\begin{equation}
\label{eq:factors_model_WLS}
\textbf{W}y = \textbf{WX}\beta + \epsilon.
\end{equation}

$\textbf{X}$ is a $(n \times p)$ full rank matrix and represents $n$ observations on each $p-1$ independent variables and an 
intercept term, $\beta$ is a $(p \times 1)$ vector of associated regression coefficients, $\textbf{W}$ is a $(n \times n)$ diagonal 
matrix with weights, $y$ is a $(n \times 1)$ vector of dependent variables and $\epsilon$ is a $(n \times 1)$ vector of errors.

The aim of the \gls{wls} is to estimate the vector of coefficients $\beta$ by minimizing the \gls{wrss}

\begin{equation}
\label{eq:factors_WRSS}
\text{WRSS} = \sum_{i=1}^{n} w_i \Big(y_i - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij}\Big)^2.
\end{equation}

A significant coefficient (meaning significantly different from 0) indicates that the associated factor had probably an influence on \gls{nfb} efficacy and the sign 
of the coefficient indicates the direction of the effect.

The second method applied was the \gls{lasso}, which naturally incorporates variable selection 
in the linear model thanks to $\ell-1$-norm applied on the coefficients. The coefficients $\hat{\beta}_j$ are obtained by minimizing the term

\begin{equation}
\label{eq:factors_lasso-minimization}
\hat{\beta} = \argmin_\beta \sum_{i=1}^{n} \Big(y_i - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij}\Big)^2 + \lambda \sum_{j=1}^{p}|\beta_{j}|,
\end{equation} 

where $\lambda$ is the regularization parameter setting more coefficients to zero as it increases. The optimal tuning parameter was determined 
by a leave-one-out cross-validation. A coefficient not set at zero means that the associated factor may have an influence on \gls{nfb} and once again,
the sign of the coefficient indicates the direction of the effect.

Eventually, the last method used to determine factors influencing \gls{nfb} was the decision tree \citep{Quinlan1986}, a non linear method. It brakes down a dataset into smaller
and smaller subsets using at each iteration a variable and a threshold chosen to optimize a simple \gls{mse} criteria 

\begin{equation}
\label{eq:factors_decision_tree_mse}
\text{MSE} = \frac{1}{n}\sum_{i=1}^{n} \Big(\hat{y}_i - {y}_i\Big)^2,
\end{equation}

with $\hat{y}$ the predicted values.
Eventually, a tree is composed of several nodes and leafs, the importance of which is decreasing from the top node. 

These methods are intrinsically different from each others, so we compared their results.
















